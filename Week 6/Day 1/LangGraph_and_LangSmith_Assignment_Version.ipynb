{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ü§ù Breakout Room #2:\n",
        "  - Part 1: LangSmith Evaluator:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "  - Part 2:\n",
        "    3. Adding Helpfulness Check and \"Loop\" Limits\n",
        "    4. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "3b97db0d-d119-4b43-b964-47291c7dba1c"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "761167a9-b570-421b-eb9c-8be3dc813f47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b0237b19-ada7-4836-edc2-228e694a5ecb"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE3 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/fab950acfbf5fea46c9313dca34ee2ae01f1728b/libs/langgraph/langgraph/prebuilt/tool_executor.py#L50) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "***Model has been fine-tuned to select the appropriate tool based on the input. Tools have descriptions which are read by the LLM and then the LLM will decide which tool to use***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "***There's no specific limit, just when the last message in the chain is not a function, that is when the \"end\" state is hit. But, we can build in a counter if we wanted and have it stop at that count.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "90f7d3dc-0fe2-4d1f-8221-9b3bcffc982e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36;1m\u001b[1;3m[0:tasks]\u001b[0m \u001b[1mStarting step 0 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3m__start__\u001b[0m -> {'messages': [HumanMessage(content='What is RAG in the context of Large Language Models? When did it break onto the scene?')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:writes]\u001b[0m \u001b[1mFinished step 0 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [HumanMessage(content='What is RAG in the context of Large Language Models? When did it break onto the scene?')]\n",
            "\u001b[36;1m\u001b[1;3m[1:tasks]\u001b[0m \u001b[1mStarting step 1 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What is RAG in the context of Large Language Models? When did it break onto the scene?', id='da8d9012-5d9e-4275-bc41-aa5647dad57a')]}\n",
            "\u001b[36;1m\u001b[1;3m[1:writes]\u001b[0m \u001b[1mFinished step 1 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content='RAG, or Retrieval-Augmented Generation, is a technique used in the context of Large Language Models (LLMs) to improve their performance by combining retrieval-based methods with generative models. The core idea behind RAG is to enhance the generative capabilities of LLMs by incorporating relevant information retrieved from a large corpus of documents or a knowledge base. This approach helps the model generate more accurate and contextually relevant responses, especially for tasks that require specific knowledge or up-to-date information.\\n\\n### Key Components of RAG:\\n1. **Retriever**: This component searches a large corpus to find relevant documents or passages based on the input query.\\n2. **Generator**: This component generates the final response by conditioning on both the input query and the retrieved documents.\\n\\n### How RAG Works:\\n1. **Query Input**: The user provides a query or prompt.\\n2. **Retrieval Step**: The retriever searches a pre-indexed corpus to find the most relevant documents or passages related to the query.\\n3. **Generation Step**: The generator takes the original query and the retrieved documents as input to generate a coherent and contextually relevant response.\\n\\n### Advantages of RAG:\\n- **Improved Accuracy**: By leveraging external knowledge, RAG can provide more accurate and detailed responses.\\n- **Up-to-date Information**: The retrieval component can access the latest information, making the model\\'s responses more current.\\n- **Contextual Relevance**: The combination of retrieval and generation helps in producing responses that are more contextually appropriate.\\n\\n### When Did RAG Break Onto the Scene?\\nRAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated the effectiveness of RAG in various knowledge-intensive tasks, such as open-domain question answering and fact-checking, showing significant improvements over previous methods.\\n\\nThe introduction of RAG marked a significant advancement in the field of natural language processing, particularly in enhancing the capabilities of large language models by integrating retrieval mechanisms.', response_metadata={'token_usage': {'completion_tokens': 424, 'prompt_tokens': 166, 'total_tokens': 590}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'stop', 'logprobs': None}, id='run-aaac31aa-45e0-40a3-a022-b2dab7ef6e85-0', usage_metadata={'input_tokens': 166, 'output_tokens': 424, 'total_tokens': 590})]\n",
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Agent Response: RAG, or Retrieval-Augmented Generation, is a technique used in the context of Large Language Models (LLMs) to improve their performance by combining retrieval-based methods with generative models. The core idea behind RAG is to enhance the generative capabilities of LLMs by incorporating relevant information retrieved from a large corpus of documents or a knowledge base. This approach helps the model generate more accurate and contextually relevant responses, especially for tasks that require specific knowledge or up-to-date information.\n",
            "\n",
            "### Key Components of RAG:\n",
            "1. **Retriever**: This component searches a large corpus to find relevant documents or passages based on the input query.\n",
            "2. **Generator**: This component generates the final response by conditioning on both the input query and the retrieved documents.\n",
            "\n",
            "### How RAG Works:\n",
            "1. **Query Input**: The user provides a query or prompt.\n",
            "2. **Retrieval Step**: The retriever searches a pre-indexed corpus to find the most relevant documents or passages related to the query.\n",
            "3. **Generation Step**: The generator takes the original query and the retrieved documents as input to generate a coherent and contextually relevant response.\n",
            "\n",
            "### Advantages of RAG:\n",
            "- **Improved Accuracy**: By leveraging external knowledge, RAG can provide more accurate and detailed responses.\n",
            "- **Up-to-date Information**: The retrieval component can access the latest information, making the model's responses more current.\n",
            "- **Contextual Relevance**: The combination of retrieval and generation helps in producing responses that are more contextually appropriate.\n",
            "\n",
            "### When Did RAG Break Onto the Scene?\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated the effectiveness of RAG in various knowledge-intensive tasks, such as open-domain question answering and fact-checking, showing significant improvements over previous methods.\n",
            "\n",
            "The introduction of RAG marked a significant advancement in the field of natural language processing, particularly in enhancing the capabilities of large language models by integrating retrieval mechanisms.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "6ae3afec-7da7-4c5d-a3ca-2ed75735c029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36;1m\u001b[1;3m[0:tasks]\u001b[0m \u001b[1mStarting step 0 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3m__start__\u001b[0m -> {'messages': [HumanMessage(content='What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:writes]\u001b[0m \u001b[1mFinished step 0 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [HumanMessage(content='What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?')]\n",
            "\u001b[36;1m\u001b[1;3m[1:tasks]\u001b[0m \u001b[1mStarting step 1 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?', id='f219264a-0695-4049-8538-5ea877f36992')]}\n",
            "\u001b[36;1m\u001b[1;3m[1:writes]\u001b[0m \u001b[1mFinished step 1 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 189, 'total_tokens': 205}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-b1b0a931-1796-4e41-ac46-98a307fef9f3-0', usage_metadata={'input_tokens': 189, 'output_tokens': 16, 'total_tokens': 205})]\n",
            "\u001b[36;1m\u001b[1;3m[2:tasks]\u001b[0m \u001b[1mStarting step 2 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3maction\u001b[0m -> {'messages': [HumanMessage(content='What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?', id='f219264a-0695-4049-8538-5ea877f36992'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 189, 'total_tokens': 205}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-b1b0a931-1796-4e41-ac46-98a307fef9f3-0', usage_metadata={'input_tokens': 189, 'output_tokens': 16, 'total_tokens': 205})]}\n",
            "\u001b[36;1m\u001b[1;3m[2:writes]\u001b[0m \u001b[1mFinished step 2 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [FunctionMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv')]\n",
            "\u001b[36;1m\u001b[1;3m[3:tasks]\u001b[0m \u001b[1mStarting step 3 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?', id='f219264a-0695-4049-8538-5ea877f36992'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 189, 'total_tokens': 205}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-b1b0a931-1796-4e41-ac46-98a307fef9f3-0', usage_metadata={'input_tokens': 189, 'output_tokens': 16, 'total_tokens': 205}),\n",
            "              FunctionMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv', id='40eb72c6-7527-4bf4-b6e4-1b8092566bee')]}\n",
            "\u001b[36;1m\u001b[1;3m[3:writes]\u001b[0m \u001b[1mFinished step 3 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Tim Dettmers bio\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 1168, 'total_tokens': 1189}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-b2241eb2-24ec-4f5c-b8da-3b604b372ac7-0', usage_metadata={'input_tokens': 1168, 'output_tokens': 21, 'total_tokens': 1189})]\n",
            "\u001b[36;1m\u001b[1;3m[4:tasks]\u001b[0m \u001b[1mStarting step 4 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3maction\u001b[0m -> {'messages': [HumanMessage(content='What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?', id='f219264a-0695-4049-8538-5ea877f36992'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 189, 'total_tokens': 205}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-b1b0a931-1796-4e41-ac46-98a307fef9f3-0', usage_metadata={'input_tokens': 189, 'output_tokens': 16, 'total_tokens': 205}),\n",
            "              FunctionMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv', id='40eb72c6-7527-4bf4-b6e4-1b8092566bee'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Tim Dettmers bio\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 1168, 'total_tokens': 1189}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-b2241eb2-24ec-4f5c-b8da-3b604b372ac7-0', usage_metadata={'input_tokens': 1168, 'output_tokens': 21, 'total_tokens': 1189})]}\n",
            "\u001b[36;1m\u001b[1;3m[4:writes]\u001b[0m \u001b[1mFinished step 4 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [FunctionMessage(content=\"Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\", name='duckduckgo_search')]\n",
            "\u001b[36;1m\u001b[1;3m[5:tasks]\u001b[0m \u001b[1mStarting step 5 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?', id='f219264a-0695-4049-8538-5ea877f36992'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 189, 'total_tokens': 205}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-b1b0a931-1796-4e41-ac46-98a307fef9f3-0', usage_metadata={'input_tokens': 189, 'output_tokens': 16, 'total_tokens': 205}),\n",
            "              FunctionMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv', id='40eb72c6-7527-4bf4-b6e4-1b8092566bee'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Tim Dettmers bio\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 1168, 'total_tokens': 1189}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-b2241eb2-24ec-4f5c-b8da-3b604b372ac7-0', usage_metadata={'input_tokens': 1168, 'output_tokens': 21, 'total_tokens': 1189}),\n",
            "              FunctionMessage(content=\"Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\", name='duckduckgo_search', id='7717e37a-e4a2-489f-b8b8-e2c3d3bc44ce')]}\n",
            "\u001b[36;1m\u001b[1;3m[5:writes]\u001b[0m \u001b[1mFinished step 5 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content=\"### What is QLoRA in Machine Learning?\\n\\nQLoRA (Quantized Low Rank Adapters) is an efficient finetuning approach designed to reduce memory usage while preserving the performance of large language models (LLMs). The key features of QLoRA include:\\n\\n1. **Memory Efficiency**: It allows finetuning of a 65 billion parameter model on a single 48GB GPU.\\n2. **Performance Preservation**: Maintains full 16-bit finetuning task performance.\\n3. **Innovations**:\\n   - **4-bit NormalFloat (NF4)**: A new data type optimized for normally distributed weights.\\n   - **Double Quantization**: Reduces memory footprint by quantizing the quantization constants.\\n   - **Paged Optimizers**: Manages memory spikes during training.\\n\\nThe approach involves backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The best model family developed using QLoRA, named Guanaco, outperforms previous openly released models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on a single GPU.\\n\\n### Technical Papers on QLoRA\\n\\n1. **Title**: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\\n   - **Authors**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\n   - **Summary**: This paper introduces QLoRA and details its innovations, performance, and applications. It also provides a detailed analysis of instruction following and chatbot performance across various datasets and model types.\\n\\n2. **Title**: [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://arxiv.org/abs/2305.14314)\\n   - **Authors**: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\n   - **Summary**: This paper proposes IR-QLoRA, which enhances the accuracy of quantized LLMs through information retention techniques.\\n\\n3. **Title**: [Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods](https://arxiv.org/abs/2305.14314)\\n   - **Authors**: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\n   - **Summary**: This paper explores the adaptation of LLMs to different domains using QLoRA and assesses the quality of fact memorization and style imitation.\\n\\n### Bio of Tim Dettmers\\n\\nTim Dettmers is a researcher focused on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. His work involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning.\", response_metadata={'token_usage': {'completion_tokens': 643, 'prompt_tokens': 1465, 'total_tokens': 2108}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'stop', 'logprobs': None}, id='run-bef8f0af-1d31-41c0-ab01-c2f3dc8b5421-0', usage_metadata={'input_tokens': 1465, 'output_tokens': 643, 'total_tokens': 2108})]\n",
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2024-05-27\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technologies derived from the perspective of unified information: (1)\n",
            "statistics-based Information Calibration Quantization allows the quantized\n",
            "parameters of LLM to retain original information accurately; (2)\n",
            "finetuning-based Information Elastic Connection makes LoRA utilizes elastic\n",
            "representation transformation with diverse information. Comprehensive\n",
            "experiments show that IR-QLoRA can significantly improve accuracy across LLaMA\n",
            "and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\n",
            "improvement on MMLU compared with the state-of-the-art methods. The significant\n",
            "performance gain requires only a tiny 0.31% additional time consumption,\n",
            "revealing the satisfactory efficiency of our IR-QLoRA. We highlight that\n",
            "IR-QLoRA enjoys excellent versatility, compatible with various frameworks\n",
            "(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\n",
            "The code is available at https://github.com/htqin/ir-qlora.\n",
            "\n",
            "Published: 2024-06-12\n",
            "Title: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\n",
            "Authors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
            "Summary: There are various methods for adapting LLMs to different domains. The most\n",
            "common methods are prompting, finetuning, and RAG. In this w\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\n",
            "\n",
            "Agent Response: ### What is QLoRA in Machine Learning?\n",
            "\n",
            "QLoRA (Quantized Low Rank Adapters) is an efficient finetuning approach designed to reduce memory usage while preserving the performance of large language models (LLMs). The key features of QLoRA include:\n",
            "\n",
            "1. **Memory Efficiency**: It allows finetuning of a 65 billion parameter model on a single 48GB GPU.\n",
            "2. **Performance Preservation**: Maintains full 16-bit finetuning task performance.\n",
            "3. **Innovations**:\n",
            "   - **4-bit NormalFloat (NF4)**: A new data type optimized for normally distributed weights.\n",
            "   - **Double Quantization**: Reduces memory footprint by quantizing the quantization constants.\n",
            "   - **Paged Optimizers**: Manages memory spikes during training.\n",
            "\n",
            "The approach involves backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The best model family developed using QLoRA, named Guanaco, outperforms previous openly released models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on a single GPU.\n",
            "\n",
            "### Technical Papers on QLoRA\n",
            "\n",
            "1. **Title**: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "   - **Summary**: This paper introduces QLoRA and details its innovations, performance, and applications. It also provides a detailed analysis of instruction following and chatbot performance across various datasets and model types.\n",
            "\n",
            "2. **Title**: [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "   - **Summary**: This paper proposes IR-QLoRA, which enhances the accuracy of quantized LLMs through information retention techniques.\n",
            "\n",
            "3. **Title**: [Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
            "   - **Summary**: This paper explores the adaptation of LLMs to different domains using QLoRA and assesses the quality of fact memorization and style imitation.\n",
            "\n",
            "### Bio of Tim Dettmers\n",
            "\n",
            "Tim Dettmers is a researcher focused on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. His work involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "1. Take state object as input and pass to agent\n",
        "2. Agent adds arxiv search function call to state object, conditional edge sees function call, and passes to action node\n",
        "3. Action node adds response to state object - in this case returns the title of paper and list of authors and passes back to agent \n",
        "4. Agent receives state object, adds duckduckgo function call to state object, conditional edge sees function call, passes to action node\n",
        "5. Action node adds response to state object - in this case searching and returning Tim Dettmer's bio and passes back to agent\n",
        "6. Agent receives state object, does not add any additional function calls, conditional edge sees no function call, goes to end state with end output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "d2b3be9d-1dbb-4109-83e4-673c9b73483e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36;1m\u001b[1;3m[0:tasks]\u001b[0m \u001b[1mStarting step 0 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3m__start__\u001b[0m -> {'messages': [HumanMessage(content='What is RAG?')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:writes]\u001b[0m \u001b[1mFinished step 0 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [HumanMessage(content='What is RAG?')]\n",
            "\u001b[36;1m\u001b[1;3m[1:tasks]\u001b[0m \u001b[1mStarting step 1 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What is RAG?', id='23eeb59d-52df-4de1-9f16-1194ae9b868e')]}\n",
            "\u001b[36;1m\u001b[1;3m[1:writes]\u001b[0m \u001b[1mFinished step 1 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content='RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here‚Äôs a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus based on the input query. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input query. This means that the information from the retrieved documents is combined with the original query to provide more context and relevant information.\\n\\n3. **Generation**: Finally, a generative model, such as a transformer-based language model (e.g., GPT-3, BERT), uses the augmented input to generate a response. The generative model can produce more accurate and contextually relevant answers because it has access to additional information from the retrieval step.\\n\\nRAG models are particularly useful in scenarios where the information needed to answer a query is not contained within the model itself but can be found in external documents. This approach leverages the strengths of both retrieval-based and generative methods, leading to more accurate and informative responses.', response_metadata={'token_usage': {'completion_tokens': 270, 'prompt_tokens': 151, 'total_tokens': 421}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None}, id='run-131a8524-8752-46a6-9c68-0b5d2108da10-0', usage_metadata={'input_tokens': 151, 'output_tokens': 270, 'total_tokens': 421})]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here‚Äôs a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus based on the input query. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input query. This means that the information from the retrieved documents is combined with the original query to provide more context and relevant information.\\n\\n3. **Generation**: Finally, a generative model, such as a transformer-based language model (e.g., GPT-3, BERT), uses the augmented input to generate a response. The generative model can produce more accurate and contextually relevant answers because it has access to additional information from the retrieval step.\\n\\nRAG models are particularly useful in scenarios where the information needed to answer a query is not contained within the model itself but can be found in external documents. This approach leverages the strengths of both retrieval-based and generative methods, leading to more accurate and informative responses.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "#Using solution dataset to save time, this was mentioned as allowed in class\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "Answers and questions are in the same order in the array. \n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "***The primary gaps in this method are that it searches by exact spelling and case, and additionally, you are not taking context into consideration.***\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "157f1019-4ee4-4994-fe80-7afc2e5fdfb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 5a1460a7' at:\n",
            "https://smith.langchain.com/o/2c5eee09-beea-5c94-a4f6-e9bdaba4058a/datasets/2fecf64e-4fff-4003-94de-1c89da12e533/compare?selectedSessions=9efad6f7-b556-410e-b8df-1374ce2b3755\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 8fe82c54 at:\n",
            "https://smith.langchain.com/o/2c5eee09-beea-5c94-a4f6-e9bdaba4058a/datasets/2fecf64e-4fff-4003-94de-1c89da12e533\n",
            "[>                                                 ] 0/6\u001b[36;1m\u001b[1;3m[0:tasks]\u001b[0m \u001b[1mStarting step 0 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3m__start__\u001b[0m -> {'messages': [HumanMessage(content='What optimizer is used in QLoRA?')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:tasks]\u001b[0m \u001b[1mStarting step 0 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3m__start__\u001b[0m -> {'messages': [HumanMessage(content='What data type was created in the QLoRA paper?')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:writes]\u001b[0m \u001b[1mFinished step 0 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [HumanMessage(content='What optimizer is used in QLoRA?')]\n",
            "\u001b[36;1m\u001b[1;3m[1:tasks]\u001b[0m \u001b[1mStarting step 1 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What optimizer is used in QLoRA?', id='62f93738-2d3b-4ac3-8df4-91fa5737d1b0')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:tasks]\u001b[0m \u001b[1mStarting step 0 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3m__start__\u001b[0m -> {'messages': [HumanMessage(content='What is a Retrieval Augmented Generation system?')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:writes]\u001b[0m \u001b[1mFinished step 0 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [HumanMessage(content='What data type was created in the QLoRA paper?')]\n",
            "\u001b[36;1m\u001b[1;3m[1:tasks]\u001b[0m \u001b[1mStarting step 1 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What data type was created in the QLoRA paper?', id='a9265566-62d9-4bf3-b150-d0b8ae83807e')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:tasks]\u001b[0m \u001b[1mStarting step 0 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3m__start__\u001b[0m -> {'messages': [HumanMessage(content='Who authored the QLoRA paper?')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:writes]\u001b[0m \u001b[1mFinished step 0 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [HumanMessage(content='What is a Retrieval Augmented Generation system?')]\n",
            "\u001b[36;1m\u001b[1;3m[1:tasks]\u001b[0m \u001b[1mStarting step 1 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What is a Retrieval Augmented Generation system?', id='32ccd6b1-edc6-49f2-9160-e972e729b67f')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:writes]\u001b[0m \u001b[1mFinished step 0 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [HumanMessage(content='Who authored the QLoRA paper?')]\n",
            "\u001b[36;1m\u001b[1;3m[1:tasks]\u001b[0m \u001b[1mStarting step 1 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='Who authored the QLoRA paper?', id='e6f59745-6457-4125-8a25-a8c7aaf689e6')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:tasks]\u001b[0m \u001b[1mStarting step 0 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3m__start__\u001b[0m -> {'messages': [HumanMessage(content='What is the most popular deep learning framework?')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:writes]\u001b[0m \u001b[1mFinished step 0 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [HumanMessage(content='What is the most popular deep learning framework?')]\n",
            "\u001b[36;1m\u001b[1;3m[1:tasks]\u001b[0m \u001b[1mStarting step 1 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What is the most popular deep learning framework?', id='aebbc4ed-0fd7-4fe5-a2fe-2435efd6fbd9')]}\n",
            "\u001b[36;1m\u001b[1;3m[1:writes]\u001b[0m \u001b[1mFinished step 1 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA data type\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 158, 'total_tokens': 176}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'function_call', 'logprobs': None}, id='run-897c7f7c-97c3-4460-b6c9-cf71afeecb4d-0', usage_metadata={'input_tokens': 158, 'output_tokens': 18, 'total_tokens': 176})]\n",
            "\u001b[36;1m\u001b[1;3m[2:tasks]\u001b[0m \u001b[1mStarting step 2 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3maction\u001b[0m -> {'messages': [HumanMessage(content='What data type was created in the QLoRA paper?', id='a9265566-62d9-4bf3-b150-d0b8ae83807e'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA data type\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 158, 'total_tokens': 176}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'function_call', 'logprobs': None}, id='run-897c7f7c-97c3-4460-b6c9-cf71afeecb4d-0', usage_metadata={'input_tokens': 158, 'output_tokens': 18, 'total_tokens': 176})]}\n",
            "\u001b[36;1m\u001b[1;3m[1:writes]\u001b[0m \u001b[1mFinished step 1 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"most popular deep learning framework 2023\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 155, 'total_tokens': 179}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-4b68344d-d3b4-4624-8838-e36adf8af7f4-0', usage_metadata={'input_tokens': 155, 'output_tokens': 24, 'total_tokens': 179})]\n",
            "\u001b[36;1m\u001b[1;3m[2:tasks]\u001b[0m \u001b[1mStarting step 2 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3maction\u001b[0m -> {'messages': [HumanMessage(content='What is the most popular deep learning framework?', id='aebbc4ed-0fd7-4fe5-a2fe-2435efd6fbd9'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"most popular deep learning framework 2023\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 155, 'total_tokens': 179}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-4b68344d-d3b4-4624-8838-e36adf8af7f4-0', usage_metadata={'input_tokens': 155, 'output_tokens': 24, 'total_tokens': 179})]}\n",
            "\u001b[36;1m\u001b[1;3m[1:writes]\u001b[0m \u001b[1mFinished step 1 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA paper\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 154, 'total_tokens': 171}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'function_call', 'logprobs': None}, id='run-df40f83d-0bbe-4a22-84a8-a767b35fbe11-0', usage_metadata={'input_tokens': 154, 'output_tokens': 17, 'total_tokens': 171})]\n",
            "\u001b[36;1m\u001b[1;3m[2:tasks]\u001b[0m \u001b[1mStarting step 2 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3maction\u001b[0m -> {'messages': [HumanMessage(content='Who authored the QLoRA paper?', id='e6f59745-6457-4125-8a25-a8c7aaf689e6'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA paper\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 154, 'total_tokens': 171}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'function_call', 'logprobs': None}, id='run-df40f83d-0bbe-4a22-84a8-a767b35fbe11-0', usage_metadata={'input_tokens': 154, 'output_tokens': 17, 'total_tokens': 171})]}\n",
            "\u001b[36;1m\u001b[1;3m[1:writes]\u001b[0m \u001b[1mFinished step 1 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content='QLoRA (Quantized Low-Rank Adaptation) typically uses the AdamW optimizer. AdamW is a variant of the Adam optimizer that includes weight decay for regularization, which helps in preventing overfitting. This optimizer is well-suited for training large language models and fine-tuning them with techniques like QLoRA.', response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 155, 'total_tokens': 223}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None}, id='run-db2fdd2f-20ff-4342-9ab3-22d80b1a4aab-0', usage_metadata={'input_tokens': 155, 'output_tokens': 68, 'total_tokens': 223})]\n",
            "\u001b[36;1m\u001b[1;3m[2:writes]\u001b[0m \u001b[1mFinished step 2 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [FunctionMessage(content=\"Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-04-01\\nTitle: Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding\\nAuthors: Lung-Chuan Chen, Zong-Ru Li\\nSummary: Large language models (LLMs) have demonstrated exceptional performance in\\nvarious NLP applications. However, the majority of existing open-source LLMs\\nare pre-trained primarily on English data and little part of other languages.\\nThis deficiency in multilingual training data results in suboptimal performance\\nwhen applied to languages with fewer available resources. Furthermore,\\nenhancing the performance of LLMs on low-resource languages by full-parameter\\nfine-tuning with additional data requires substantial computational resources,\\nposing computational barriers for research organizations and individual\\nresearchers. Consequently, several techniques such as parameter-efficient\\ntuning and advanced embedding initialization have been proposed to address\\nthese challenges. In this work, we combine them to facilitate cross-lingual\\ntransfer on English-dominated open-source LLM. To effectively enhance the\\nmodel's proficiency in Traditional Chinese, we conduct secondary pre-training\\non Llama 2 7B with Traditional Chinese data by leveraging QLoRA and our\\nproposed zip-tie embedding initialization. The resulting model called Bailong,\\nwhich stands for Bilingual trAnsfer learnIng based on qLOra and zip-tie\\nembeddiNG. We present Bailong-instruct 7B, a fine-tuned version of Bailong 7B\\noptimized for multi-turn dialogue scenarios. Recognizing the inadequacy of\\nbenchmark datasets in Traditional Chinese, we further introduce Bailong-bench\\nto assess the alignment of models with human preferences and the capability to\\nfollow instructions in both Traditional Chinese and English tasks. In our\\nevaluation, Bailong-instruct 7B exhibits competitive performance on\\nBailong-bench and other benchmark datasets when compared to other open-source\\nmodels of similar or even larger parameter sizes. Bailong-instruct 7B and\\nBailong-bench are public\", name='arxiv')]\n",
            "\u001b[36;1m\u001b[1;3m[3:tasks]\u001b[0m \u001b[1mStarting step 3 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What data type was created in the QLoRA paper?', id='a9265566-62d9-4bf3-b150-d0b8ae83807e'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA data type\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 158, 'total_tokens': 176}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'function_call', 'logprobs': None}, id='run-897c7f7c-97c3-4460-b6c9-cf71afeecb4d-0', usage_metadata={'input_tokens': 158, 'output_tokens': 18, 'total_tokens': 176}),\n",
            "              FunctionMessage(content=\"Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-04-01\\nTitle: Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding\\nAuthors: Lung-Chuan Chen, Zong-Ru Li\\nSummary: Large language models (LLMs) have demonstrated exceptional performance in\\nvarious NLP applications. However, the majority of existing open-source LLMs\\nare pre-trained primarily on English data and little part of other languages.\\nThis deficiency in multilingual training data results in suboptimal performance\\nwhen applied to languages with fewer available resources. Furthermore,\\nenhancing the performance of LLMs on low-resource languages by full-parameter\\nfine-tuning with additional data requires substantial computational resources,\\nposing computational barriers for research organizations and individual\\nresearchers. Consequently, several techniques such as parameter-efficient\\ntuning and advanced embedding initialization have been proposed to address\\nthese challenges. In this work, we combine them to facilitate cross-lingual\\ntransfer on English-dominated open-source LLM. To effectively enhance the\\nmodel's proficiency in Traditional Chinese, we conduct secondary pre-training\\non Llama 2 7B with Traditional Chinese data by leveraging QLoRA and our\\nproposed zip-tie embedding initialization. The resulting model called Bailong,\\nwhich stands for Bilingual trAnsfer learnIng based on qLOra and zip-tie\\nembeddiNG. We present Bailong-instruct 7B, a fine-tuned version of Bailong 7B\\noptimized for multi-turn dialogue scenarios. Recognizing the inadequacy of\\nbenchmark datasets in Traditional Chinese, we further introduce Bailong-bench\\nto assess the alignment of models with human preferences and the capability to\\nfollow instructions in both Traditional Chinese and English tasks. In our\\nevaluation, Bailong-instruct 7B exhibits competitive performance on\\nBailong-bench and other benchmark datasets when compared to other open-source\\nmodels of similar or even larger parameter sizes. Bailong-instruct 7B and\\nBailong-bench are public\", name='arxiv', id='7b8917a7-ceb2-4dfb-914e-e8554042a5b5')]}\n",
            "\u001b[36;1m\u001b[1;3m[2:writes]\u001b[0m \u001b[1mFinished step 2 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [FunctionMessage(content=\"TensorFlow is a Python-based, open-source deep learning framework supported by Google, which also integrates well with third-party platforms. It has an active community contributing to updates and offering guidance and support. In fact, it's one of the most popular deep learning frameworks, used by major corporations like Airbnb, Intel, and ... Let us move on to check out the 7 best deep learning frameworks you should know in 2024. 1. TensorFlow. TensorFlow is one of the most popular, open-source libraries that is being heavily used for numerical computation deep learning. Google introduced it in 2015 for their internal RnD work but later when they saw the capabilities of this ... 9 Top Deep Learning Frameworks. Machine learning is a crucial technical area that has grown tremendously over the past 10 years. For instance, the worldwide machine learning market, which was estimated at $15.44 billion in 2021, is projected to rise at a CAGR of 38.8% from $21.17 billion to $209.91 billion by 2029 as a result of the rising use of technological innovations. Keras. Francois Chollet originally developed Keras, with 350,000+ users and 700+ open-source contributors, making it one of the fastest-growing deep learning framework packages. Keras supports high-level neural network API, written in Python. What makes Keras interesting is that it runs on top of TensorFlow, Theano, and CNTK. PyTorch is the de facto research framework with most SOTA models. It offers features essential for research, like GPU capabilities, an easy API, scalability, and excellent debugging tools. However, in Reinforcement Learning (RL), TensorFlow might be better due to its native agents' library and DeepMind's Acme. 2.\", name='duckduckgo_search')]\n",
            "\u001b[36;1m\u001b[1;3m[3:tasks]\u001b[0m \u001b[1mStarting step 3 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What is the most popular deep learning framework?', id='aebbc4ed-0fd7-4fe5-a2fe-2435efd6fbd9'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"most popular deep learning framework 2023\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 155, 'total_tokens': 179}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-4b68344d-d3b4-4624-8838-e36adf8af7f4-0', usage_metadata={'input_tokens': 155, 'output_tokens': 24, 'total_tokens': 179}),\n",
            "              FunctionMessage(content=\"TensorFlow is a Python-based, open-source deep learning framework supported by Google, which also integrates well with third-party platforms. It has an active community contributing to updates and offering guidance and support. In fact, it's one of the most popular deep learning frameworks, used by major corporations like Airbnb, Intel, and ... Let us move on to check out the 7 best deep learning frameworks you should know in 2024. 1. TensorFlow. TensorFlow is one of the most popular, open-source libraries that is being heavily used for numerical computation deep learning. Google introduced it in 2015 for their internal RnD work but later when they saw the capabilities of this ... 9 Top Deep Learning Frameworks. Machine learning is a crucial technical area that has grown tremendously over the past 10 years. For instance, the worldwide machine learning market, which was estimated at $15.44 billion in 2021, is projected to rise at a CAGR of 38.8% from $21.17 billion to $209.91 billion by 2029 as a result of the rising use of technological innovations. Keras. Francois Chollet originally developed Keras, with 350,000+ users and 700+ open-source contributors, making it one of the fastest-growing deep learning framework packages. Keras supports high-level neural network API, written in Python. What makes Keras interesting is that it runs on top of TensorFlow, Theano, and CNTK. PyTorch is the de facto research framework with most SOTA models. It offers features essential for research, like GPU capabilities, an easy API, scalability, and excellent debugging tools. However, in Reinforcement Learning (RL), TensorFlow might be better due to its native agents' library and DeepMind's Acme. 2.\", name='duckduckgo_search', id='bd8bedd0-3282-4c45-a4bf-e53ad33c8ceb')]}\n",
            "\u001b[36;1m\u001b[1;3m[2:writes]\u001b[0m \u001b[1mFinished step 2 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [FunctionMessage(content='Published: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-02-16\\nTitle: QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\\nAuthors: Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh\\nSummary: Finetuning large language models requires huge GPU memory, restricting the\\nchoice to acquire Larger models. While the quantized version of the Low-Rank\\nAdaptation technique, named QLoRA, significantly alleviates this issue, finding\\nthe efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a\\npre-defined rank and, therefore, cannot be reconfigured for its lower ranks\\nwithout requiring further fine-tuning steps. This paper proposes QDyLoRA\\n-Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach\\nfor dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to\\nefficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables\\nfine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one\\nround of fine-tuning. Experimental results show that QDyLoRA is competitive to\\nQLoRA and outperforms when employing its optimal rank.\\n\\nPublished: 2023-12-31\\nTitle: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nAuthors: Dipankar Sarkar\\nSummary: This paper aims to introduce and analyze the Viz system in a comprehensive\\nway, a novel system architecture that integrates Quantized Low-Rank Adapters\\n(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\\nresource efficient marketplace. Viz represents a significant contribution to\\nthe field of artificial intelligence, particularly in addressing the challenges\\nof computational efficiency, legal compliance, and economic sustainability in\\nthe utilization and monetization of LLMs. The paper delineates the scholarly\\ndiscourse and developments that have informed the creation of Viz, focusing\\nprimarily on the advancements in LLM models, copyright issues in AI training\\n(NYT case, 2023), and the evolution of model fine-tuning techniques,\\nparticularly low-rank adapters and quantized low-rank adapters, to create a\\nsustainable and economically compliant framework for LLM utilization. The\\neconomic model it proposes benefits content creators, AI developers, and\\nend-users, delineating a harmonious integration of technology,', name='arxiv')]\n",
            "\u001b[36;1m\u001b[1;3m[3:tasks]\u001b[0m \u001b[1mStarting step 3 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='Who authored the QLoRA paper?', id='e6f59745-6457-4125-8a25-a8c7aaf689e6'),\n",
            "              AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA paper\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 154, 'total_tokens': 171}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'function_call', 'logprobs': None}, id='run-df40f83d-0bbe-4a22-84a8-a767b35fbe11-0', usage_metadata={'input_tokens': 154, 'output_tokens': 17, 'total_tokens': 171}),\n",
            "              FunctionMessage(content='Published: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-02-16\\nTitle: QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\\nAuthors: Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh\\nSummary: Finetuning large language models requires huge GPU memory, restricting the\\nchoice to acquire Larger models. While the quantized version of the Low-Rank\\nAdaptation technique, named QLoRA, significantly alleviates this issue, finding\\nthe efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a\\npre-defined rank and, therefore, cannot be reconfigured for its lower ranks\\nwithout requiring further fine-tuning steps. This paper proposes QDyLoRA\\n-Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach\\nfor dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to\\nefficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables\\nfine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one\\nround of fine-tuning. Experimental results show that QDyLoRA is competitive to\\nQLoRA and outperforms when employing its optimal rank.\\n\\nPublished: 2023-12-31\\nTitle: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nAuthors: Dipankar Sarkar\\nSummary: This paper aims to introduce and analyze the Viz system in a comprehensive\\nway, a novel system architecture that integrates Quantized Low-Rank Adapters\\n(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\\nresource efficient marketplace. Viz represents a significant contribution to\\nthe field of artificial intelligence, particularly in addressing the challenges\\nof computational efficiency, legal compliance, and economic sustainability in\\nthe utilization and monetization of LLMs. The paper delineates the scholarly\\ndiscourse and developments that have informed the creation of Viz, focusing\\nprimarily on the advancements in LLM models, copyright issues in AI training\\n(NYT case, 2023), and the evolution of model fine-tuning techniques,\\nparticularly low-rank adapters and quantized low-rank adapters, to create a\\nsustainable and economically compliant framework for LLM utilization. The\\neconomic model it proposes benefits content creators, AI developers, and\\nend-users, delineating a harmonious integration of technology,', name='arxiv', id='b00c767c-1e26-44b3-ab25-fb5abad69553')]}\n",
            "\u001b[36;1m\u001b[1;3m[3:writes]\u001b[0m \u001b[1mFinished step 3 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content='In the QLoRA paper, the authors introduced a new data type called **4-bit NormalFloat (NF4)**. This data type is designed to be information-theoretically optimal for normally distributed weights, which helps in reducing memory usage while preserving performance during the finetuning of large language models.', response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 1071, 'total_tokens': 1133}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None}, id='run-952e6086-aef6-446b-bbd3-78542f9382ac-0', usage_metadata={'input_tokens': 1071, 'output_tokens': 62, 'total_tokens': 1133})]\n",
            "\u001b[36;1m\u001b[1;3m[3:writes]\u001b[0m \u001b[1mFinished step 3 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content='The QLoRA paper titled \"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\" was authored by:\\n\\n- Haotong Qin\\n- Xudong Ma\\n- Xingyu Zheng\\n- Xiaoyang Li\\n- Yang Zhang\\n- Shouda Liu\\n- Jie Luo\\n- Xianglong Liu\\n- Michele Magno\\n\\nYou can find more details and the code for the paper at [this GitHub repository](https://github.com/htqin/ir-qlora).', response_metadata={'token_usage': {'completion_tokens': 109, 'prompt_tokens': 1117, 'total_tokens': 1226}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None}, id='run-b248f20b-a9bf-429c-9e93-bba6b985ba57-0', usage_metadata={'input_tokens': 1117, 'output_tokens': 109, 'total_tokens': 1226})]\n",
            "\u001b[36;1m\u001b[1;3m[3:writes]\u001b[0m \u001b[1mFinished step 3 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content=\"As of 2023, the most popular deep learning frameworks are:\\n\\n1. **TensorFlow**: Developed by Google, TensorFlow is one of the most widely used open-source libraries for numerical computation and deep learning. It has a large and active community, making it a popular choice for both research and production.\\n\\n2. **PyTorch**: Developed by Facebook's AI Research lab, PyTorch is particularly popular in the research community. It offers features essential for research, such as GPU capabilities, an easy-to-use API, scalability, and excellent debugging tools.\\n\\n3. **Keras**: Originally developed by Fran√ßois Chollet, Keras is a high-level neural network API written in Python. It is known for its simplicity and ease of use, and it runs on top of TensorFlow, Theano, and CNTK.\\n\\nThese frameworks are widely adopted by major corporations and research institutions, making them the top choices for deep learning projects.\", response_metadata={'token_usage': {'completion_tokens': 190, 'prompt_tokens': 557, 'total_tokens': 747}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None}, id='run-b1afa9a6-0bfb-483f-bf06-1edad3a3e979-0', usage_metadata={'input_tokens': 557, 'output_tokens': 190, 'total_tokens': 747})]\n",
            "\u001b[36;1m\u001b[1;3m[1:writes]\u001b[0m \u001b[1mFinished step 1 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content='A Retrieval-Augmented Generation (RAG) system is a type of artificial intelligence model that combines retrieval-based and generation-based approaches to improve the quality and relevance of generated text. Here‚Äôs a breakdown of the two components:\\n\\n1. **Retrieval-Based Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus or database. It uses techniques such as information retrieval, search algorithms, or embeddings to find the most relevant content based on the input query.\\n\\n2. **Generation-Based Component**: This part of the system generates text based on the retrieved information. It typically uses models like GPT (Generative Pre-trained Transformer) or other neural network-based language models to produce coherent and contextually appropriate text.\\n\\n### How RAG Works\\n\\n1. **Input Query**: The system receives an input query or prompt from the user.\\n2. **Retrieval**: The retrieval component searches a large corpus to find documents or snippets that are relevant to the query.\\n3. **Augmentation**: The retrieved information is then used to augment the input query, providing additional context and content.\\n4. **Generation**: The generation component uses the augmented input to produce a response or generate text that is informed by the retrieved information.\\n\\n### Benefits of RAG\\n\\n- **Improved Relevance**: By incorporating relevant information from a large corpus, the system can generate more accurate and contextually appropriate responses.\\n- **Knowledge Integration**: It can integrate up-to-date information from external sources, making it more knowledgeable and versatile.\\n- **Enhanced Performance**: Combining retrieval and generation can lead to better performance on tasks such as question answering, summarization, and conversational AI.\\n\\n### Applications\\n\\n- **Question Answering Systems**: Providing precise answers by retrieving relevant documents and generating a concise response.\\n- **Conversational Agents**: Enhancing chatbots and virtual assistants with more accurate and context-aware responses.\\n- **Content Creation**: Assisting in writing articles, reports, or other content by providing relevant information and generating coherent text.\\n\\nOverall, RAG systems leverage the strengths of both retrieval and generation to create more powerful and effective AI applications.', response_metadata={'token_usage': {'completion_tokens': 434, 'prompt_tokens': 155, 'total_tokens': 589}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None}, id='run-62762e1a-2916-47cf-8441-a5ae03acad82-0', usage_metadata={'input_tokens': 155, 'output_tokens': 434, 'total_tokens': 589})]\n",
            "[------->                                          ] 1/6\u001b[36;1m\u001b[1;3m[0:tasks]\u001b[0m \u001b[1mStarting step 0 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3m__start__\u001b[0m -> {'messages': [HumanMessage(content='What significant improvements does the LoRA system make?')]}\n",
            "\u001b[36;1m\u001b[1;3m[0:writes]\u001b[0m \u001b[1mFinished step 0 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [HumanMessage(content='What significant improvements does the LoRA system make?')]\n",
            "\u001b[36;1m\u001b[1;3m[1:tasks]\u001b[0m \u001b[1mStarting step 1 with 1 task:\n",
            "\u001b[0m- \u001b[32;1m\u001b[1;3magent\u001b[0m -> {'messages': [HumanMessage(content='What significant improvements does the LoRA system make?', id='81a1aea6-c1ce-4570-a138-237c5275a9a8')]}\n",
            "[------------------------>                         ] 3/6\u001b[36;1m\u001b[1;3m[1:writes]\u001b[0m \u001b[1mFinished step 1 with writes to 1 channel:\n",
            "\u001b[0m- \u001b[33;1m\u001b[1;3mmessages\u001b[0m -> [AIMessage(content=\"LoRA, or Low-Rank Adaptation, is a technique used in the field of machine learning, particularly in the context of fine-tuning large language models. Here are some significant improvements and benefits that LoRA provides:\\n\\n1. **Parameter Efficiency**: LoRA significantly reduces the number of trainable parameters required for fine-tuning. Instead of updating all the parameters of a large model, LoRA introduces low-rank matrices that adapt the pre-trained model's weights. This makes the fine-tuning process much more parameter-efficient.\\n\\n2. **Memory Efficiency**: By reducing the number of parameters that need to be updated, LoRA also reduces the memory footprint during training. This is particularly beneficial when working with very large models that would otherwise require substantial computational resources.\\n\\n3. **Faster Training**: With fewer parameters to update, the training process becomes faster. This can lead to quicker iterations and more rapid experimentation, which is valuable in research and development settings.\\n\\n4. **Cost-Effectiveness**: The reduction in computational and memory requirements translates to lower costs for training and deploying models. This makes it more feasible for organizations with limited resources to leverage large language models.\\n\\n5. **Maintaining Performance**: Despite the reduction in parameters, LoRA has been shown to maintain or even improve the performance of the fine-tuned models. This means that the efficiency gains do not come at the cost of model accuracy or effectiveness.\\n\\n6. **Flexibility**: LoRA can be applied to various types of neural network architectures and is not limited to a specific model type. This makes it a versatile tool for a wide range of applications.\\n\\n7. **Scalability**: The approach is scalable and can be applied to models of different sizes, making it suitable for both small and large-scale machine learning projects.\\n\\nOverall, LoRA provides a more efficient and cost-effective way to fine-tune large language models without compromising on performance, making it a valuable technique in the field of machine learning.\", response_metadata={'token_usage': {'completion_tokens': 401, 'prompt_tokens': 156, 'total_tokens': 557}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'stop', 'logprobs': None}, id='run-30d9aed9-5c39-44a2-aa20-a5dad87745a3-0', usage_metadata={'input_tokens': 156, 'output_tokens': 401, 'total_tokens': 557})]\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.COT Contextual Accuracy</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.0</td>\n",
              "      <td>4.00</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>d10f93b8-692f-4435-a407-775a15fdb2e2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.073220</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.538721</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.628710</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.409929</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.101306</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.343559</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.670881</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.COT Contextual Accuracy  \\\n",
              "count                    6.0                              4.00   \n",
              "unique                   NaN                               NaN   \n",
              "top                      NaN                               NaN   \n",
              "freq                     NaN                               NaN   \n",
              "mean                     1.0                              0.75   \n",
              "std                      0.0                              0.50   \n",
              "min                      1.0                              0.00   \n",
              "25%                      1.0                              0.75   \n",
              "50%                      1.0                              1.00   \n",
              "75%                      1.0                              1.00   \n",
              "max                      1.0                              1.00   \n",
              "\n",
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      6     0        6.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                    False   NaN             NaN   \n",
              "freq                       3   NaN             NaN   \n",
              "mean                     NaN   NaN        4.073220   \n",
              "std                      NaN   NaN        1.538721   \n",
              "min                      NaN   NaN        1.628710   \n",
              "25%                      NaN   NaN        3.409929   \n",
              "50%                      NaN   NaN        4.101306   \n",
              "75%                      NaN   NaN        5.343559   \n",
              "max                      NaN   NaN        5.670881   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     d10f93b8-692f-4435-a407-775a15fdb2e2  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 5a1460a7',\n",
              " 'results': {'b1a78087-9bc2-43c5-94db-7f3201c901ac': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and concise answer to the question asked. It not only states that the AdamW optimizer is used in QLoRA, but also provides additional information about the optimizer, such as its features and why it is suitable for QLoRA. This additional information can be very helpful for someone who is not familiar with the AdamW optimizer or why it would be used in this context. \\n\\nTherefore, the submission is helpful, insightful, and appropriate, meeting the given criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1600b4f0-b5ca-465f-a185-509c4a7c11f5'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=None, value=None, comment=\"The context does not provide any information about the optimizer used in QLoRA. However, the student's answer mentions that QLoRA uses the AdamW optimizer. Without any information from the context to contradict or confirm this, it's impossible to definitively grade the student's answer.\\nGRADE: UNDECIDABLE\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9c22b178-f752-41ac-abb1-b4b566e825b5'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('19c55356-de2d-4af9-9f17-54060ef22e68'), target_run_id=None)],\n",
              "   'execution_time': 1.62871,\n",
              "   'run_id': 'd10f93b8-692f-4435-a407-775a15fdb2e2',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) typically uses the AdamW optimizer. AdamW is a variant of the Adam optimizer that includes weight decay for regularization, which helps in preventing overfitting. This optimizer is well-suited for training large language models and fine-tuning them with techniques like QLoRA.',\n",
              "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
              "  'dd3d340f-ced0-41d1-a00e-21de74ffd961': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and direct answer to the question, stating that the QLoRA paper introduced a new data type called 4-bit NormalFloat (NF4). \\n\\nIn addition to providing the direct answer, the submission also provides additional information about the purpose and function of the new data type, explaining that it is designed to be optimal for normally distributed weights and helps in reducing memory usage while preserving performance during the finetuning of large language models. \\n\\nThis additional information is relevant and insightful, as it provides context and understanding about why the new data type was created and what it is used for. \\n\\nTherefore, the submission is helpful, insightful, and appropriate, meeting the given criterion. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9d32b7e4-1f3c-4390-9ecb-3fba6c4e4211'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context mentions 'NF4' and 'NormalFloat' as the data type created in the QLoRA paper. The student's answer also mentions '4-bit NormalFloat (NF4)' as the data type created in the QLoRA paper. The student's answer matches the context. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('95639bed-f526-454e-8735-395b892139be'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d4b8f0a5-614e-402e-907c-d53cd85e2480'), target_run_id=None)],\n",
              "   'execution_time': 3.292798,\n",
              "   'run_id': 'f38a3c15-f83e-4d44-85dd-dbd6096dd214',\n",
              "   'output': 'In the QLoRA paper, the authors introduced a new data type called **4-bit NormalFloat (NF4)**. This data type is designed to be information-theoretically optimal for normally distributed weights, which helps in reducing memory usage while preserving performance during the finetuning of large language models.',\n",
              "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
              "  '88a788a2-1b94-46f9-ba05-5f313d51e3f2': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". To assess this, we need to consider whether the submission provides useful, insightful, and appropriate information in response to the input query.\\n\\n1. **Useful Information**: The submission provides a comprehensive explanation of what a Retrieval-Augmented Generation (RAG) system is. It breaks down the two main components of the system, explains how it works, discusses its benefits, and provides examples of its applications. This information is useful for anyone seeking to understand RAG systems.\\n\\n2. **Insightful Information**: The submission goes beyond a basic definition of a RAG system. It provides insights into how the system works, the advantages it offers, and its potential applications. This depth of information adds value to the submission and makes it insightful.\\n\\n3. **Appropriate Information**: The information provided in the submission is directly related to the input query. It stays on topic and provides relevant information about RAG systems. Therefore, it is appropriate.\\n\\nBased on this analysis, the submission meets the criterion of being helpful. It provides useful, insightful, and appropriate information in response to the input query.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4290d376-c851-4562-b054-1974287913fc'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=None, value=None, comment=\"The context provided does not contain any information about Retrieval Augmented Generation systems. Therefore, it is impossible to determine the accuracy of the student's answer based on the provided context. GRADE: CANNOT BE DETERMINED\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('316f86a3-2f2a-466e-b4fa-5d87dcbdc3f1'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ecf07375-e60e-4b59-ac39-0eaa71597caa'), target_run_id=None)],\n",
              "   'execution_time': 5.644316,\n",
              "   'run_id': '57a8c1fe-c9c4-4e76-8a72-62e3b246f9d5',\n",
              "   'output': 'A Retrieval-Augmented Generation (RAG) system is a type of artificial intelligence model that combines retrieval-based and generation-based approaches to improve the quality and relevance of generated text. Here‚Äôs a breakdown of the two components:\\n\\n1. **Retrieval-Based Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus or database. It uses techniques such as information retrieval, search algorithms, or embeddings to find the most relevant content based on the input query.\\n\\n2. **Generation-Based Component**: This part of the system generates text based on the retrieved information. It typically uses models like GPT (Generative Pre-trained Transformer) or other neural network-based language models to produce coherent and contextually appropriate text.\\n\\n### How RAG Works\\n\\n1. **Input Query**: The system receives an input query or prompt from the user.\\n2. **Retrieval**: The retrieval component searches a large corpus to find documents or snippets that are relevant to the query.\\n3. **Augmentation**: The retrieved information is then used to augment the input query, providing additional context and content.\\n4. **Generation**: The generation component uses the augmented input to produce a response or generate text that is informed by the retrieved information.\\n\\n### Benefits of RAG\\n\\n- **Improved Relevance**: By incorporating relevant information from a large corpus, the system can generate more accurate and contextually appropriate responses.\\n- **Knowledge Integration**: It can integrate up-to-date information from external sources, making it more knowledgeable and versatile.\\n- **Enhanced Performance**: Combining retrieval and generation can lead to better performance on tasks such as question answering, summarization, and conversational AI.\\n\\n### Applications\\n\\n- **Question Answering Systems**: Providing precise answers by retrieving relevant documents and generating a concise response.\\n- **Conversational Agents**: Enhancing chatbots and virtual assistants with more accurate and context-aware responses.\\n- **Content Creation**: Assisting in writing articles, reports, or other content by providing relevant information and generating coherent text.\\n\\nOverall, RAG systems leverage the strengths of both retrieval and generation to create more powerful and effective AI applications.',\n",
              "   'reference': {'must_mention': ['ground', 'context']}},\n",
              "  'd6d4b0ad-e796-4f86-9ae6-f00a56334ceb': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed answer to the question, listing all the authors of the QLoRA paper. This is helpful for anyone looking for this specific information. \\n\\nMoreover, the submission goes beyond the asked question and provides additional information such as the full title of the paper and a link to the GitHub repository where more details and the code for the paper can be found. This additional information is insightful and appropriate, as it provides further resources for anyone interested in the QLoRA paper.\\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('87d73b65-49bf-42eb-ad5d-845d415916c6'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"The context provided only mentions two names, 'Tim' and 'Dettmers', as the authors of the QLoRA paper. The student's answer lists a number of different authors, none of which match the names provided in the context. Therefore, based on the information given, the student's answer is incorrect.\\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a5bbfb60-ea0f-4575-a2c7-9f94a3eebc33'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d33d8d4e-ff23-43a1-b52c-202612c06775'), target_run_id=None)],\n",
              "   'execution_time': 3.761322,\n",
              "   'run_id': '9f67e644-e86f-4597-bb80-2bb57b11095c',\n",
              "   'output': 'The QLoRA paper titled \"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\" was authored by:\\n\\n- Haotong Qin\\n- Xudong Ma\\n- Xingyu Zheng\\n- Xiaoyang Li\\n- Yang Zhang\\n- Shouda Liu\\n- Jie Luo\\n- Xianglong Liu\\n- Michele Magno\\n\\nYou can find more details and the code for the paper at [this GitHub repository](https://github.com/htqin/ir-qlora).',\n",
              "   'reference': {'must_mention': ['Tim', 'Dettmers']}},\n",
              "  '558c0c4a-643e-4704-914a-5d5f5571c37e': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed answer to the question, \"What is the most popular deep learning framework?\" Instead of providing just one framework, the submission lists the top three most popular deep learning frameworks as of 2023, which are TensorFlow, PyTorch, and Keras. \\n\\nFor each framework, the submission provides a brief description, including who developed it, its main features, and why it is popular. This information is helpful for someone who is not familiar with these frameworks and wants to understand why they are popular.\\n\\nThe submission is also insightful as it provides additional context about the adoption of these frameworks by major corporations and research institutions. This information can help the reader understand the practical applications and relevance of these frameworks in the real world.\\n\\nThe submission is appropriate as it directly answers the question and provides relevant and accurate information. It does not include any inappropriate or irrelevant content.\\n\\nBased on this analysis, the submission meets the criterion of helpfulness.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('74b79a53-c482-45e9-a884-c37c8a47e8d3'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer mentions both TensorFlow and PyTorch, which are the two deep learning frameworks provided in the context. The student also provides additional information about each framework, including their developers and key features. The student also mentions Keras, which is not mentioned in the context, but this does not contradict the information in the context. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9080a812-eb0f-456b-897a-a5d2d7315e78'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('cf480642-e1c9-40e3-9a54-ff4aeb6435ac'), target_run_id=None)],\n",
              "   'execution_time': 4.44129,\n",
              "   'run_id': '7acc421d-2ea7-4fef-ab32-1377c06d9d4b',\n",
              "   'output': \"As of 2023, the most popular deep learning frameworks are:\\n\\n1. **TensorFlow**: Developed by Google, TensorFlow is one of the most widely used open-source libraries for numerical computation and deep learning. It has a large and active community, making it a popular choice for both research and production.\\n\\n2. **PyTorch**: Developed by Facebook's AI Research lab, PyTorch is particularly popular in the research community. It offers features essential for research, such as GPU capabilities, an easy-to-use API, scalability, and excellent debugging tools.\\n\\n3. **Keras**: Originally developed by Fran√ßois Chollet, Keras is a high-level neural network API written in Python. It is known for its simplicity and ease of use, and it runs on top of TensorFlow, Theano, and CNTK.\\n\\nThese frameworks are widely adopted by major corporations and research institutions, making them the top choices for deep learning projects.\",\n",
              "   'reference': {'must_mention': ['PyTorch', 'TensorFlow']}},\n",
              "  '931475d2-ec6c-48c5-9a88-19e617704651': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of the LoRA system and the improvements it brings. The answer is structured in a clear and concise manner, with each point explaining a different benefit of the LoRA system. This makes the answer helpful as it provides a comprehensive understanding of the topic.\\n\\nThe submission is also insightful. It not only lists the improvements but also explains how these improvements impact the use of large language models in machine learning. This gives the reader a deeper understanding of the significance of these improvements.\\n\\nLastly, the submission is appropriate. It directly answers the question asked in the input and stays on topic throughout. The language used is professional and suitable for the context.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0a311e02-0aab-428e-9d84-9ab44dce7874'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context provided is 'reduce' and 'parameters'. The student's answer mentions that the LoRA system significantly reduces the number of trainable parameters required for fine-tuning. This directly addresses the context provided. The student's answer also provides additional benefits of the LoRA system, none of which contradict the context or the question. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('48103d64-00cd-4875-b962-8e886441afc4'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4d8ab3a6-5a98-4193-92c3-60278b68c060'), target_run_id=None)],\n",
              "   'execution_time': 5.670881,\n",
              "   'run_id': '1e0234ca-3bad-4b18-ba1b-c406dc6bca65',\n",
              "   'output': \"LoRA, or Low-Rank Adaptation, is a technique used in the field of machine learning, particularly in the context of fine-tuning large language models. Here are some significant improvements and benefits that LoRA provides:\\n\\n1. **Parameter Efficiency**: LoRA significantly reduces the number of trainable parameters required for fine-tuning. Instead of updating all the parameters of a large model, LoRA introduces low-rank matrices that adapt the pre-trained model's weights. This makes the fine-tuning process much more parameter-efficient.\\n\\n2. **Memory Efficiency**: By reducing the number of parameters that need to be updated, LoRA also reduces the memory footprint during training. This is particularly beneficial when working with very large models that would otherwise require substantial computational resources.\\n\\n3. **Faster Training**: With fewer parameters to update, the training process becomes faster. This can lead to quicker iterations and more rapid experimentation, which is valuable in research and development settings.\\n\\n4. **Cost-Effectiveness**: The reduction in computational and memory requirements translates to lower costs for training and deploying models. This makes it more feasible for organizations with limited resources to leverage large language models.\\n\\n5. **Maintaining Performance**: Despite the reduction in parameters, LoRA has been shown to maintain or even improve the performance of the fine-tuned models. This means that the efficiency gains do not come at the cost of model accuracy or effectiveness.\\n\\n6. **Flexibility**: LoRA can be applied to various types of neural network architectures and is not limited to a specific model type. This makes it a versatile tool for a wide range of applications.\\n\\n7. **Scalability**: The approach is scalable and can be applied to models of different sizes, making it suitable for both small and large-scale machine learning projects.\\n\\nOverall, LoRA provides a more efficient and cost-effective way to fine-tune large language models without compromising on performance, making it a valuable technique in the field of machine learning.\",\n",
              "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!\n",
        "\n",
        "***We are using the GPT-4 model to evaluate the final answer vs. the initial question and then determine whether the final answer is \"extremely helpful\", and then return a Y if so, a N if not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "***Following code defines new graph with three nodes. Agent, action, and a passthrough node***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "***Following code sets the entry node as the agent node***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "***Defines conditional edges. First conditional edge determines whether the agent node passes to action node or passthrough node. Second conditional edge determines whether passthrough node sends to agent or ends.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    check_helpfulness,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### Following code defines Non conditional edge, action node is always passing back to agent node###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "***Following code compiles graph***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "***following code invokes graph with human message as given below***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "72096ca6-e78e-475a-dde5-fe076c5b776d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\n",
            "\n",
            "\n",
            "Agent Response: Let's break down each of these terms and individuals:\n",
            "\n",
            "### LoRA (Low-Rank Adaptation)\n",
            "LoRA stands for Low-Rank Adaptation. It is a technique used in machine learning, particularly in the context of fine-tuning large pre-trained models. The main idea behind LoRA is to adapt a pre-trained model to a new task by introducing a low-rank decomposition of the weight updates. This allows for efficient fine-tuning with fewer parameters, which can be particularly useful when dealing with large models that are computationally expensive to train.\n",
            "\n",
            "### Tim Dettmers\n",
            "Tim Dettmers is a researcher known for his work in the field of machine learning, particularly in the areas of efficient training and inference of large neural networks. He has contributed to the development of techniques that make it more feasible to train and deploy large models, such as those used in natural language processing. His work often focuses on optimizing the computational aspects of machine learning to make it more accessible and efficient.\n",
            "\n",
            "### Attention\n",
            "Attention is a mechanism used in neural networks, particularly in the context of sequence-to-sequence models like those used in natural language processing. The attention mechanism allows the model to focus on different parts of the input sequence when generating each part of the output sequence. This is particularly useful in tasks like machine translation, where the model needs to align words in the source language with words in the target language.\n",
            "\n",
            "The most well-known form of attention is the \"self-attention\" mechanism used in Transformer models. In self-attention, each word in the input sequence is compared with every other word to compute a set of attention weights. These weights determine how much focus the model should place on each word when generating the output. This allows the model to capture long-range dependencies and relationships between words, which is crucial for understanding and generating coherent text.\n",
            "\n",
            "Would you like more detailed information on any of these topics?\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "1c29765b-42fc-449f-dce4-62dcb747a03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: What is prompt engineering and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Prompt engineering is a concept primarily associated with the field of artificial intelligence (AI) and natural language processing (NLP). It involves the design and crafting of prompts or input queries to elicit desired responses from AI models, particularly large language models like GPT-3, GPT-4, and others. The goal of prompt engineering is to optimize the interaction with these models to achieve specific outcomes, whether for generating text, answering questions, or performing other tasks.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Crafting Effective Prompts**: Designing prompts that guide the AI to produce accurate, relevant, and coherent responses.\n",
            "2. **Iterative Refinement**: Continuously refining prompts based on the responses received to improve the quality and relevance of the output.\n",
            "3. **Understanding Model Behavior**: Gaining insights into how different prompts influence the model's behavior and output.\n",
            "4. **Application-Specific Prompts**: Tailoring prompts for specific applications, such as customer support, content generation, or educational tools.\n",
            "\n",
            "### Emergence of Prompt Engineering:\n",
            "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The capabilities of these models to generate human-like text based on prompts highlighted the importance of crafting effective prompts to harness their full potential. Researchers, developers, and practitioners began to explore and document techniques for prompt engineering to improve the performance and utility of these models in various applications.\n",
            "\n",
            "The concept has since evolved and gained traction as more advanced models have been developed, and as the AI community has recognized the critical role of prompt design in leveraging the power of these models.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is RAG and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: RAG stands for Retrieval-Augmented Generation. It is a framework that combines the strengths of retrieval-based and generation-based models to improve the performance of natural language processing tasks, particularly in the context of question answering and information retrieval.\n",
            "\n",
            "### Key Components of RAG:\n",
            "1. **Retriever**: This component is responsible for fetching relevant documents or passages from a large corpus based on the input query.\n",
            "2. **Generator**: This component generates a coherent and contextually appropriate response based on the retrieved documents.\n",
            "\n",
            "### How RAG Works:\n",
            "1. **Query Input**: A user inputs a query.\n",
            "2. **Document Retrieval**: The retriever fetches relevant documents or passages from a pre-indexed corpus.\n",
            "3. **Response Generation**: The generator uses the retrieved documents to generate a response that is both relevant and contextually appropriate.\n",
            "\n",
            "### Advantages of RAG:\n",
            "- **Improved Accuracy**: By leveraging external documents, RAG can provide more accurate and contextually relevant answers.\n",
            "- **Scalability**: It can handle large corpora of documents, making it suitable for applications requiring extensive knowledge bases.\n",
            "- **Flexibility**: It can be fine-tuned for specific domains or tasks, enhancing its versatility.\n",
            "\n",
            "### When Did RAG Break Onto the Scene?\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated the effectiveness of RAG in various knowledge-intensive tasks, setting a new benchmark in the field of natural language processing.\n",
            "\n",
            "Would you like more detailed information or specific aspects of RAG?\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is fine-tuning and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new, often smaller, dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large dataset, making it more efficient and effective for specialized tasks.\n",
            "\n",
            "### Key Aspects of Fine-Tuning:\n",
            "1. **Pre-trained Model**: A model that has already been trained on a large and diverse dataset.\n",
            "2. **Target Task**: The specific task or domain for which the model needs to be adapted.\n",
            "3. **Additional Training**: The process of training the pre-trained model on the new dataset, often with a lower learning rate to avoid overfitting.\n",
            "\n",
            "### Benefits of Fine-Tuning:\n",
            "- **Efficiency**: Reduces the computational resources and time required compared to training a model from scratch.\n",
            "- **Performance**: Often leads to better performance on the target task due to the transfer of learned features from the pre-trained model.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of deep learning and the availability of large pre-trained models. Some key milestones include:\n",
            "\n",
            "- **2012**: The success of AlexNet in the ImageNet competition demonstrated the power of deep learning and pre-trained models.\n",
            "- **2014**: The introduction of transfer learning techniques in natural language processing (NLP) with models like Word2Vec and GloVe.\n",
            "- **2018**: The release of BERT (Bidirectional Encoder Representations from Transformers) by Google, which popularized fine-tuning in NLP. BERT was pre-trained on a large corpus and then fine-tuned for various NLP tasks, setting new benchmarks in the field.\n",
            "\n",
            "Fine-tuning has since become a standard practice in both computer vision and NLP, enabling the development of highly specialized models with relatively modest amounts of task-specific data.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is LLM-based agents and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: LLM-based agents, or Large Language Model-based agents, are artificial intelligence systems that leverage large language models to perform a variety of tasks. These tasks can include natural language understanding, text generation, translation, summarization, question answering, and more. The core technology behind these agents is typically a deep learning model trained on vast amounts of text data to understand and generate human-like text.\n",
            "\n",
            "### Key Characteristics of LLM-based Agents:\n",
            "1. **Natural Language Processing (NLP):** They excel in understanding and generating human language.\n",
            "2. **Contextual Understanding:** They can maintain context over longer conversations or documents.\n",
            "3. **Versatility:** They can be fine-tuned for specific tasks or domains.\n",
            "4. **Scalability:** They can handle a wide range of applications from chatbots to complex data analysis.\n",
            "\n",
            "### Timeline of Development:\n",
            "- **Pre-2018:** Early NLP models like Word2Vec, GloVe, and initial versions of recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) were used for language tasks.\n",
            "- **2018:** The introduction of the Transformer architecture by Vaswani et al. revolutionized NLP. This led to the development of models like BERT (Bidirectional Encoder Representations from Transformers) by Google.\n",
            "- **2019:** OpenAI released GPT-2 (Generative Pre-trained Transformer 2), which demonstrated the potential of large-scale language models in generating coherent and contextually relevant text.\n",
            "- **2020:** OpenAI released GPT-3, a significantly larger and more powerful model with 175 billion parameters. GPT-3's capabilities in generating human-like text and performing a wide range of tasks brought significant attention to LLM-based agents.\n",
            "- **2021 and Beyond:** Continued advancements in LLMs, including models like Google's LaMDA, OpenAI's Codex, and others, have further pushed the boundaries of what these agents can do. They have been integrated into various applications, from customer service chatbots to advanced research tools.\n",
            "\n",
            "### Breakthrough Moment:\n",
            "The release of GPT-3 in June 2020 is often considered the breakthrough moment for LLM-based agents. Its ability to perform a wide array of tasks with minimal fine-tuning and its impressive text generation capabilities captured the imagination of both the public and the tech community, leading to widespread interest and adoption.\n",
            "\n",
            "### Applications:\n",
            "- **Customer Support:** Automated chatbots and virtual assistants.\n",
            "- **Content Creation:** Writing articles, generating marketing copy, and creative writing.\n",
            "- **Education:** Tutoring systems and personalized learning assistants.\n",
            "- **Healthcare:** Assisting in medical documentation and patient interaction.\n",
            "- **Research:** Summarizing scientific papers and generating hypotheses.\n",
            "\n",
            "In summary, LLM-based agents have become a significant part of the AI landscape, with their development marked by key advancements in NLP and the introduction of powerful models like GPT-3.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print_messages(messages)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
